{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c611d5",
   "metadata": {},
   "source": [
    "# Deep-Learning with Keras\n",
    "\n",
    "#### Ugur URESIN, AI Engineer | Data Scientist\n",
    "#### Mail: uresin.ugur@gmail.com\n",
    "\n",
    "## Chapter 02. Neural Networks - Image Classification\n",
    "\n",
    "### Dataset: MNIST\n",
    "\n",
    "### The Steps\n",
    "* **Step01**. Loading Dataset\n",
    "* **Step02**. Assesing the Dataset\n",
    "* **Step03**. Building the network architecture\n",
    "* **Step04**. The compilation\n",
    "* **Step05**. Data preperation\n",
    "* **Step06**. Preparing the labels (categorical transformation)\n",
    "* **Step07**. Training\n",
    "* **Step08**. Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bdea8",
   "metadata": {},
   "source": [
    "#### Step 01. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde52a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72f033c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7794",
   "metadata": {},
   "source": [
    "#### Step 02. Assesing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "074873a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training image dimensions: (60000, 28, 28)\n",
      "Number of training labels: 60000\n",
      "Test image dimensions: (10000, 28, 28)\n",
      "Number of test labels: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training image dimensions:\", train_images.shape)\n",
    "print(\"Number of training labels:\", len(train_labels))\n",
    "print(\"Test image dimensions:\", test_images.shape)\n",
    "print(\"Number of test labels:\", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03216586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's look at a one image\n",
    "IMG_INDEX = 1  # change to look at other images\n",
    "\n",
    "plt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\n",
    "plt.xlabel(train_labels[IMG_INDEX])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810d688",
   "metadata": {},
   "source": [
    "#### Step03. Building the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87378bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "Here, the network (model) consists of a sequence of two Dense layers.\n",
    "Dense layers are also called fully connected layers.\n",
    "The second layer is a 10-way softmax layer, which means it will return an array of 10 probability scores (sum=1).\n",
    "Each score will be the probability that the current digit image belongs to one of 10 digit classes.\n",
    "'''\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation=\"relu\", input_shape=(28*28,))) \n",
    "model.add(layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2660260f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ddfd8ef89a71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_ch02.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \"\"\"\n\u001b[1;32m    239\u001b[0m     dot = model_to_dot(model, show_shapes, show_layer_names, rankdir,\n\u001b[0;32m--> 240\u001b[0;31m                        expand_nested, dpi)\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mextension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0m_check_pydot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dashed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36m_check_pydot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpydot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         raise ImportError(\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;34m'Failed to import `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;34m'Please install `pydot`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             'For example with `pip install pydot`.')\n",
      "\u001b[0;31mImportError\u001b[0m: Failed to import `pydot`. Please install `pydot`. For example with `pip install pydot`."
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(model, to_file='model_ch02.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba6af7",
   "metadata": {},
   "source": [
    "#### Step04. The compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32fe066",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"rmsprop\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262c1bb",
   "metadata": {},
   "source": [
    "#### Step05. Data preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16737271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-Shaping the Images\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "\n",
    "#Normalizing the Images for Faster Converging\n",
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f8eb4e",
   "metadata": {},
   "source": [
    "#### Step06. Preparing the labels (categorical transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb6a613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc41ae",
   "metadata": {},
   "source": [
    "#### Step07. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42661a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.2575 - accuracy: 0.9255\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.1034 - accuracy: 0.9691\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 2s 5ms/step - loss: 0.0680 - accuracy: 0.9793\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0494 - accuracy: 0.9851\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 3s 5ms/step - loss: 0.0369 - accuracy: 0.9893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28c6b24e1f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9b975",
   "metadata": {},
   "source": [
    "#### Step08. Evaluating the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40d1f5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9797\n",
      "test_acc: 0.9797000288963318\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99431525",
   "metadata": {},
   "source": [
    "* The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the training set accuracy.  \n",
    "* This gap between training accuracy and test accuracy is an example of **overfitting**.  \n",
    "* It's a the fact that machine-learning models tend to perform worse on new data than on their training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
