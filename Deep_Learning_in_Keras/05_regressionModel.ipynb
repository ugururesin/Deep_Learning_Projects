{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c611d5",
   "metadata": {},
   "source": [
    "# Deep-Learning with Keras\n",
    "\n",
    "#### Ugur URESIN, AI Engineer | Data Scientist\n",
    "#### Mail: uresin.ugur@gmail.com\n",
    "\n",
    "## Chapter 05. Neural Networks - Regression\n",
    "### Dataset: Boston Housing\n",
    "\n",
    "### The Steps\n",
    "* **Step01**. Loading Dataset\n",
    "* **Step02**. Assesing the Dataset\n",
    "* **Step03**. Preparing the Dataset\n",
    "* **Step04**. Building the network architecture\n",
    "* **Step05**. The compilation\n",
    "* **Step06**. Creating a Validation Set\n",
    "* **Step07**. Training\n",
    "* **Step08**. Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63bdea8",
   "metadata": {},
   "source": [
    "### Step 01. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bde52a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import boston_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644a7794",
   "metadata": {},
   "source": [
    "### Step 02. Assesing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074873a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data dimensions: (404, 13)\n",
      "Number of training labels: 404\n",
      "Test data dimensions: (102, 13)\n",
      "Number of test labels: 102\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data dimensions:\", train_data.shape)\n",
    "print(\"Number of training labels:\", len(train_targets))\n",
    "print(\"Test data dimensions:\", test_data.shape)\n",
    "print(\"Number of test labels:\", len(test_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f86da4d",
   "metadata": {},
   "source": [
    "### Step03. Preparing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc2c0f",
   "metadata": {},
   "source": [
    "#### Normalizing the datasets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dead73ea",
   "metadata": {},
   "source": [
    "It would be problematic to feed into a neural network values that all take wildly different ranges.  \n",
    "The network might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult.  \n",
    "A widespread best practice to deal with such data is to do **feature-wise normalization**.  \n",
    "\n",
    "For each feature in the input data (a column in the input data matrix),  \n",
    "the mean of the feature is substracted and divided by the standard deviation,  \n",
    "so that the feature is centered around 0 and has a unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70481f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "std = train_data.std(axis=0)\n",
    "\n",
    "train_data -= mean\n",
    "train_data /= std\n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603dec9b",
   "metadata": {},
   "source": [
    "Note that the quantities used **for normalizing the test data** are computed **using the\n",
    "training data**!!!  \n",
    "\n",
    "You should **never use any quantity computed on the test data**, even for something as simple as data normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d810d688",
   "metadata": {},
   "source": [
    "### Step04. Building the network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87378bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) #mae:mean-absolute-error\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0d4289",
   "metadata": {},
   "source": [
    "The network ends with a single unit and **no activation** (it will be a linear layer).   \n",
    "This is a typical setup for scalar regression (a regression where you’re trying to predict a single continuous value).\n",
    "\n",
    "A new metric is monitored during training: **mean absolute error (MAE)**.  \n",
    "It’s the absolute value of the difference between the predictions and the targets.  \n",
    "For instance, an MAE of 0.5 on this problem would mean your predictions are off by $500 on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8da33c",
   "metadata": {},
   "source": [
    "### Step06. The Compilation and Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793b90f6",
   "metadata": {},
   "source": [
    "#### k-fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d785b16",
   "metadata": {},
   "source": [
    "![k-fold validation](./img/kfold_Validation.png \"k-fold validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "985ff166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 1\n",
      "processing fold # 2\n",
      "processing fold # 3\n",
      "processing fold # 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 4\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "    print('processing fold #', i+1)\n",
    "    \n",
    "    #preparing the validation data, from k'th partition of the data\n",
    "    val_data    = train_data[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    val_targets = train_targets[i*num_val_samples: (i+1)*num_val_samples]\n",
    "    \n",
    "    #preparing the training data: data from all other partitions\n",
    "    partial_train_data = np.concatenate([train_data[:i * num_val_samples],\n",
    "                                         train_data[(i + 1) * num_val_samples:]],\n",
    "                                        axis=0)\n",
    "    \n",
    "    partial_train_targets = np.concatenate([train_targets[:i * num_val_samples],\n",
    "                                            train_targets[(i + 1) * num_val_samples:]],\n",
    "                                           axis=0)\n",
    "    \n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8998a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:  [2.086811065673828, 2.220487117767334, 2.6658310890197754, 2.27522349357605]\n",
      "Mean score:  2.312088191509247\n"
     ]
    }
   ],
   "source": [
    "print(\"Scores: \", all_scores)\n",
    "print(\"Mean score: \", np.mean(all_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
