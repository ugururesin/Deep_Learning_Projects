{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c611d5",
   "metadata": {},
   "source": [
    "# Deep-Learning with Keras\n",
    "\n",
    "#### Ugur URESIN, AI Engineer | Data Scientist\n",
    "#### Mail: uresin.ugur@gmail.com\n",
    "\n",
    "## Chapter 09. Recurrent Neural Nets for Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6fcf7",
   "metadata": {},
   "source": [
    "### Using Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ecee96",
   "metadata": {},
   "source": [
    "Another popular and powerful way to associate a vector with a word is the use of **dense word vectors** (a.k.a **word embeddings**).  \n",
    "\n",
    "**One-hot encoding** are binary, sparse (mostly made of zeros), and very high-dimensional!  \n",
    "\n",
    "**Word embeddings** are low- dimensional floating-point vectors (that is, dense vectors, as opposed to sparse vec- tors).  \n",
    "See figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419bd4a",
   "metadata": {},
   "source": [
    "![one-hot vs. word-embeddings](./img/word-vectors.png \"one-hot vs. word-embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b6a08",
   "metadata": {},
   "source": [
    "Unlike the word vectors obtained via one-hot encoding, **word embeddings are learned from data**.  \n",
    "\n",
    "It’s common to see word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional when dealing with very large vocabularies.  \n",
    "On the other hand, one-hot encoding words generally leads to vectors that are 20,000 dimensional or greater.  \n",
    "So, word embeddings pack more information into far fewer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2996c8",
   "metadata": {},
   "source": [
    "### Instantiating an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14f7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "embedding_layer = Embedding(1000,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b60d6e",
   "metadata": {},
   "source": [
    "#### Loading the IMDB data for use with an Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf02940b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ugururessn/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/datasets/imdb.py:101: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/Users/ugururessn/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/keras/datasets/imdb.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "max_features = 10000 #number of words to consider as features\n",
    "maxlen = 20 #cuts off the text after this numeber of words (among the max_features most common words)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "    num_words = max_features) #loads the data as list of integers\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen) #turns lists of integers into a 2D int tensor\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e02cf8",
   "metadata": {},
   "source": [
    "#### Using an Embedding layer and classifier on the IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43b6953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ugururessn/opt/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 210us/step - loss: 0.6691 - acc: 0.6203 - val_loss: 0.6190 - val_acc: 0.6976\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 206us/step - loss: 0.5406 - acc: 0.7517 - val_loss: 0.5235 - val_acc: 0.7276\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 178us/step - loss: 0.4600 - acc: 0.7853 - val_loss: 0.4980 - val_acc: 0.7478\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 168us/step - loss: 0.4220 - acc: 0.8084 - val_loss: 0.4902 - val_acc: 0.7550\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.3959 - acc: 0.8214 - val_loss: 0.4924 - val_acc: 0.7572\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 184us/step - loss: 0.3754 - acc: 0.8344 - val_loss: 0.4956 - val_acc: 0.7580\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 175us/step - loss: 0.3565 - acc: 0.8465 - val_loss: 0.5000 - val_acc: 0.7564\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.3388 - acc: 0.8559 - val_loss: 0.5055 - val_acc: 0.7538\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 186us/step - loss: 0.3211 - acc: 0.8671 - val_loss: 0.5119 - val_acc: 0.7552\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 204us/step - loss: 0.3035 - acc: 0.8773 - val_loss: 0.5194 - val_acc: 0.7518\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8 , input_length=maxlen))\n",
    "'''\n",
    "Specifies the maximum input length to the Embedding layer so you can later flatten the embedded inputs.\n",
    "After the Embedding layer, the activations have shape (samples, maxlen, 8).\n",
    "'''\n",
    "model.add(Flatten()) #flattens the 3D tensor of embedding into a 2D tensor (shape=sample, maxlen*8)\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59a71d14",
   "metadata": {},
   "source": [
    "You get to a validation accuracy of ~76%, which is pretty good considering that you’re only looking at the first 20 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and sentence structure (for example, this model would likely treat both “this movie is a bomb” and “this movie is the bomb” as being negative reviews). It’s much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044f123",
   "metadata": {},
   "source": [
    "### From raw text to word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334855a",
   "metadata": {},
   "source": [
    "#### Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff784df",
   "metadata": {},
   "source": [
    "First go to http://mng.bz/0tIo and download the raw IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcead5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "imdb_dir = './data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708cec92",
   "metadata": {},
   "source": [
    "#### Tokenizing the text of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59edd66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100               #Cuts off reviews after 100 words\n",
    "training_samples = 200     #Trains on 200 samples\n",
    "validation_samples = 10000 #Validates on 10,000 samples\n",
    "max_words = 10000          #Considers only the top 10,000 words in the dataset          \n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "indices = np.arange(data.shape[0])\n",
    "'''\n",
    "Splits the data into a training set and a validation set, but first shuffles the data,\n",
    "because you’re starting with data in which samples are ordered (all negative first, then all positive)\n",
    "'''\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
